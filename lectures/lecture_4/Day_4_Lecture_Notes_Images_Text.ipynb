{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Handling Images and Text\n",
    "## Introduction to Data Science\n",
    "### Kigali, Rwanda\n",
    "### July 11th, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. Representing Image Data\n",
    "2. Convolutional Neural Networks\n",
    "3. Representing Text Data\n",
    "4. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Representing Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do we Represent Gray-scale Images as Numerical Data?\n",
    "\n",
    "<img src=\"./fig/fig0.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent an Image as a Vector?\n",
    "<img src=\"./fig/fig1.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent an Image as a Vector?\n",
    "<img src=\"./fig/fig2.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent an Image as a Vector?\n",
    "<img src=\"./fig/fig3.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent an Image as a Vector?\n",
    "<img src=\"./fig/fig4.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent an Image as a Vector?\n",
    "\n",
    "This image, when flattened, is represented as a numpy array of shape `(441, )`.\n",
    "<img src=\"./fig/fig5.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent a Color Image Numerically?\n",
    "<img src=\"./fig/fig6.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges of Modeling with Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with Image Data is Challenging\n",
    "\n",
    "In applications involving images, the first task is often to parse an image into a set of 'features' that are relevant for the task at hand. That is, we prefer not to work with images as a set of pixels.\n",
    "\n",
    "**Question:** Can you think of why?\n",
    "\n",
    "<img src=\"./fig/fig8.jpg\" style='height:300px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Data Based Tasks \n",
    "\n",
    "<img src=\"./fig/fig7.png\" style='height:300px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Extraction with Neural Networks\n",
    "\n",
    "**Goal:** find a way to represent images as a set of \"features\".\n",
    "\n",
    "Formally, a ***feature***, $F$, is an image represented as an array. \n",
    "\n",
    "We want to learn a function $h$ mapping an image $X$ to a set of $K$ features $[F_1, F_2, \\ldots, F_K]$.\n",
    "\n",
    "That is, we want to learn a neural network, called a **convolutional neural network**, to represent such a function $h$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutions and Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Layers\n",
    "A convolutional neural network typically consists of feature extracting layers and condensing layers.\n",
    "\n",
    "The feature extracting layers are called **convolutional layers**, each node in these layers uses a small fixed set of weights to transform the image in the following way:\n",
    "\n",
    "<img src=\"./fig/fig9.gif\" style=\"width: 500px;\" align=\"center\"/>\n",
    "\n",
    "This set of fixed weights for each node in the convolutional layer is often called a ***filter*** or a ***kernel***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Connections to Classical Image Processing\n",
    "The term \"filter\" comes from image processing where one has standard ways to transforms raw images:\n",
    "<img src=\"./fig/fig10.png\" style=\"width: 300px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Do Filters Do?\n",
    "\n",
    "For example, to blur an image, we can pass an $n\\times n$ filter over the image, replacing each pixel with the average value of its neighbours in the $n\\times n$ window. The larger the window, the more intense the blurring effect. This corresponds to the Box Blur filter, e.g. $\\frac{1}{9}\\left(\\begin{array}{ccc}1 & 1 & 1\\\\ 1 & 1 & 1 \\\\1 & 1 & 1\\end{array}\\right)$:\n",
    "\n",
    "<img src=\"./fig/fig11.png\" style=\"width: 600px;\" align=\"center\"/>\n",
    "\n",
    "In an Gaussian blur, for each pixel, closer neighbors have a stronger effect on the value of the pixel (i.e. we take a weighted average of neighboring pixel values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling Layers\n",
    "\n",
    "Often in CNN's we include a **pooling layer** after a convolutional layer. In a pooling layer, we 'condense' small regions in the convolved image:\n",
    "\n",
    "<img src=\"./fig/fig12.gif\" style=\"width: 600px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Extraction for Classification\n",
    "\n",
    "We know that we want to learn the weights of a CNN for feature extraction, but what should our training objective be?\n",
    "\n",
    "**Goal:** We should learn to extract features that best helps us to perform our downstream task (classification).\n",
    "\n",
    "**Idea:** We train a CNN for feature extraction and a model (e.g. MLP, decision tree, logistic regression) for classification, *simultaneously* and *end-to-end*.\n",
    "\n",
    "<img src=\"./fig/fig13.png\" style=\"width: 800px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementing a Convolutional Neural Network in `keras`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Networks for Image Classification\n",
    "\n",
    "``` python\n",
    "# image shape\n",
    "image_shape = (64, 64)\n",
    "# Stride size\n",
    "stride_size = (2, 2)\n",
    "# Pool size\n",
    "pool_size = (2, 2)\n",
    "# Number of filters\n",
    "filters = 2\n",
    "# Kernel size\n",
    "kernel_size = (5, 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Networks for Image Classification\n",
    "\n",
    "``` python \n",
    "cnn_model = Sequential()\n",
    "# feature extraction layer 0: convolution\n",
    "cnn_model.add(Conv2D(filters, kernel_size=kernel_size, padding='same',\n",
    "                     activation='tanh',\n",
    "                     input_shape=(image_shape[0], image_shape[1], 1)))\n",
    "# feature extraction layer 1: max pooling\n",
    "cnn_model.add(MaxPooling2D(pool_size=pool_size, strides=stride_size))\n",
    "\n",
    "# input to classification layers: flattening\n",
    "cnn_model.add(Flatten())\n",
    "\n",
    "# classification layer 0: dense non-linear transformation\n",
    "cnn_model.add(Dense(10, activation='tanh'))\n",
    "# classification layer 3: output label probability\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model \n",
    "cnn_model.compile(optimizer='Adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Pre-Trained CNNs with Any Classifier\n",
    "\n",
    "You can use a number of pretrained CNNs for feature extraction (https://keras.io/applications/), and then use these features as input for any classifier (e.g. random forest, decision tree, MLP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Natural Language Processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Levels of Linguistic Knowledge in NLP\n",
    "\n",
    "Natural language processing deals with building models/algorithms to automatically analyze and represent human language.\n",
    "\n",
    "<img src=\"./fig/fig14.png\" style='height:300px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP: Tasks and Applications\n",
    "\n",
    "**Tasks:** do this...\n",
    "1. Classify entire texts\n",
    "2. Classify individual words\n",
    "  - Parts of speech tagging\n",
    "  - Chunking\n",
    "  -  Parsing/stemming\n",
    "  - Semantic rolel abeling\n",
    "5. Generating text\n",
    "  - Speech recognition\n",
    "  - Machine translation \n",
    "  - Summarization\n",
    "\n",
    "**Applications:** in order to...\n",
    "1. Classify document: spam, sentiment, etc\n",
    "2. Auto-complete and auto-correct\n",
    "3. Build conversational agents/dialogue systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main Challenges\n",
    "\n",
    "1. Ambiguity all all levels: ‘I made her duck’, ‘I went to the bank...’\n",
    "2. Language changes through time, across domains\n",
    "3. Information retrieval\n",
    "4. Many rare words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Typical Approach\n",
    "\n",
    "All NLP solutions involve three phases:\n",
    "\n",
    "1. Create a representation of the text\n",
    "2. Extract ‘important features’\n",
    "3. Build a (statistical) machine learning model to accomplish task using these features\n",
    "\n",
    "Traditionally, the features are manually and task-specifically engineered. More recently, task-agnostic ways of learning ‘important features’ have become possible with highly flexible models like neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How Do We Represent Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing Textual Data\n",
    "Comparing the content of the following two sentences is easy for an English speaking human (clearly both are discussing the same topic, but with different emotional undertone):\n",
    "\n",
    "1. Linear R3gr3ssion is very very cool!\n",
    "2. What don’t I like it a single bit? Linear regressing!\n",
    "\n",
    "But a computer doesn’t understand\n",
    "  - which words are nouns, verbs etc (grammar)\n",
    "  - how to find the topic (word ordering)\n",
    "  - feeling expressed in each sentence (sentiment)\n",
    "We need to represent the sentences in formats that a computer can easily process and manipulate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "If we’re interested in the topics/content of text, we may find many components of English sentences to be uninformative.\n",
    "\n",
    "1. Word ordering\n",
    "2. Punctuation\n",
    "3. Conjugation of verbs (go vs going), declension of nouns (chair vs chairs)\n",
    "4. Capitalization\n",
    "5. Words with mostly grammatical functions: prepositions (before, under), articles (the, a, an) etc\n",
    "6. Pronouns?\n",
    "\n",
    "These uninformative features of text will only confuse and distract a machine and should be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing Documents: Bag Of Words\n",
    "\n",
    "After preprocessing our sentences:\n",
    "\n",
    "1. (**S1**) linear regression is very very cool\n",
    "2. (**S2**) what don’t like single bit linear regression\n",
    "\n",
    "We represent text in the format that is most accessible to a computer: numeric. \n",
    "\n",
    "We simply make a vector of the counts of the words in each sentence.\n",
    "\n",
    "<img src=\"./fig/fig15.png\" style='height:100px;'>\n",
    "\n",
    "\n",
    "Turning a piece of text into a vector of word counts is called ***Bag of Words***.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of Words Representation in `python`\n",
    "\n",
    "``` python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# define documents\n",
    "corpus = ['Well done!', 'Good work, good!',  'Excellent!', 'Poor effort!', 'not good', 'poor work', 'Could have done better on this.']\n",
    "\n",
    "# vectorize text\n",
    "vectorizer = CountVectorizer(stop_words=['on', 'this'], min_df=0., max_df=1.)\n",
    "x = vectorizer.fit_transform(corpus).toarray()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Document Classification Using Bag of Words\n",
    "<img src=\"./fig/fig16.png\" style='height:300px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Do the Hidden Layers in the Network Mean?\n",
    "The hidden layers of the document classifying are representations of words are **low-dimensional** real-valued vectors. These vectors are called ***word embeddings***. Sometimes, these representation captures semantic information!\n",
    "<img src=\"./fig/fig17.png\" style='height:400px;'>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
