{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Models for Classification\n",
    "## Introduction to Data Science\n",
    "### Kigali, Rwanda\n",
    "### July 8th, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. What is Classification\n",
    "2. Logistic Regression\n",
    "3. Interpreting and Evaluating Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Classification?\n",
    "\n",
    "A regression problem is where we predict a **quantitative** outcome $y$ based on some covariates $x$.\n",
    "\n",
    "   - **Example:** Given the number of room, the number of allowed guests and the neighborhood, predict price of an Airbnb listing.\n",
    "\n",
    "A ***classification*** problem is a **categorical** outcome $y$ based on some covariates $x$.\n",
    "\n",
    "   - **Example:** Given the age, education and income level of a loan applicant, predict whether or not the loan application will be approved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Role of Decision Boundaries\n",
    "Suppose our data has a small number of predictors, by visualizing the data we can intuitively check how easy it is to separate the classes.\n",
    "<img src=\"fig/fig0.png\" alt=\"\" style=\"height: 300px;\"/>\n",
    "Ideally, the classes are easily separated by a curve (or surface) in the input space, this curve (or surface) is called the ***decision boundary***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Decision Boundaries\n",
    "\n",
    "When the decision boundary is linear, it is defined by the equation\n",
    "$$\n",
    "\\mathbf{w}^\\top \\mathbf{x} = w_0x_0 + w_1x_1 + \\ldots + w_D x_D = 0\n",
    "$$\n",
    "where $x_0 = 1$. Often we write $b = w_0x_0$ and we call $b$ the ***bias*** term.\n",
    "\n",
    "The vector $\\mathbf{w}$ allow us to gauge the 'distance' of a point from the decision boundary\n",
    "<img src=\"fig/fig1.png\" alt=\"\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hard Decisions or Soft\n",
    "\n",
    "Once we have a decision boundary should we always classify points on one side of the boundary as 1 and points on the other side as 0?\n",
    "\n",
    "<img src=\"fig/fig2.png\" alt=\"\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression: Modeling Probabilities of Labels\n",
    "\n",
    "To model the probability of labeling a point a certain class, we have to convert distance, $\\mathbf{w}^\\top\\mathbf{x}$ (which is unbounded) into a number between 0 and 1, using the ***sigmoid function***:\n",
    "$$\n",
    "\\text{Prob}(y = 1 | \\mathbf{x}) = \\text{sigmoid}(\\underbrace{\\mathbf{w}^\\top\\mathbf{x}}_{\\text{distance}})\n",
    "$$\n",
    "where $\\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "\n",
    "To fit our model, we **choose** to learn the parameters of $\\mathbf{w}$ that maximizes the likelihood of our training data. This is equivalent to minimizing ***binary cross-entropy***:\n",
    "$$\n",
    "\\min_{\\mathbf{w}}\\, \\mathrm{CrossEnt}(\\mathbf{w}) =\\min_{\\mathbf{w}} \\sum^N_{n=1} y_n \\log( \\mathbf{w}^\\top\\mathbf{x}_n) + (1 - y_n) \\log\\left(1 - \\mathbf{w}^\\top\\mathbf{x}_n\\right)\n",
    "$$\n",
    "\n",
    "Unfortunately, we cannot solve for the minimum of the binary cross-entropy loss analytically! \n",
    "\n",
    "Tomorrow we will see how to minimize this loss function algorithmically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression in `sklearn`\n",
    "\n",
    "``` python\n",
    "# import the logistic regression model from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# create a logistic regression model with linear boundary\n",
    "logistic = linear_model.LogisticRegression(C=1.)\n",
    "# fit our logistic regression model, i.e. find the parameters w \n",
    "# that maximizes the likelihood of x_train, y_train\n",
    "logistic.fit(x_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hard Predictions with Logistic Regression\n",
    "Remember that logistic regression predicts the **probability** that `y=1`:\n",
    "$$\n",
    "\\text{Prob}(y = 1 | \\mathbf{x}) = \\text{sigmoid}(\\underbrace{\\mathbf{w}^\\top\\mathbf{x}}_{\\text{distance}})\n",
    "$$\n",
    "But what if we have to make a hard decision `y=1` or `y=0`?\n",
    "\n",
    "To do this, we **choose** a threshhold, say 0.5, and decide:\n",
    "\\begin{align}\n",
    "y=1 \\text{ if } \\mathrm{Prob}(y = 1 | \\mathbf{x}) > 0.5\\\\\n",
    "y=0 \\text{ if } \\mathrm{Prob}(y = 1 | \\mathbf{x}) < 0.5\n",
    "\\end{align}\n",
    "\n",
    "In `sklearn`:\n",
    "``` python\n",
    "logistic.predict(x_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpreting and Evaluating Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpreting Logistic Regression\n",
    "\n",
    "Suppose that you fit a logistic regression model to predict whether a loan application should be approved. Suppose that you have three attributes: \n",
    "\n",
    "1. `x_1` representing gender: 0 for male, 1 for female\n",
    "2. `x_2` for the income\n",
    "3. `x_3` for the loan amount\n",
    "\n",
    "Suppose that the parameters you found are:\n",
    "$$\n",
    "p(y=1 | x_1, x_2, x_3) = \\mathrm{sigmoid}(-1 + 3 x_1 + 1.5 x_2 + 1.75 x_3).\n",
    "$$\n",
    "\n",
    "What are the parameters telling us about the most influential attribute for predicting loan approval? What does this say about our data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Classifiers Using Accuracy\n",
    "\n",
    "Suppose your train/test accuracies are as follows:\n",
    "\n",
    "<img src=\"fig/fig4.png\" alt=\"\" style=\"height: 250px;\"/>\n",
    "\n",
    "Would you say that your classifier is fitting the data well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Classifiers by Visualizing the Decision Boundary\n",
    "\n",
    "Suppose you visualize the decision boundary:\n",
    "\n",
    "<img src=\"fig/fig3.png\" alt=\"\" style=\"height: 300px;\"/>\n",
    "\n",
    "Would you say that your classifier is fitting the data well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Classifiers by Computing the Confusion Matrix\n",
    "\n",
    "A confusion matrix breaks down the performance of a classifier into categories.\n",
    "<img src=\"fig/fig6.png\" alt=\"\" style=\"height: 270px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Classifiers by Computing the Confusion Matrix\n",
    "\n",
    "Suppose the confusion matrix of your classifier is:\n",
    "\n",
    "<img src=\"fig/fig7.png\" alt=\"\" style=\"height: 150px;\"/>\n",
    "\n",
    "Would you say that your classifier fit the data well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Addressing Class Imbalance: Reweighting\n",
    "When classes in your data set are extremely unbalanced, the models you train can be unincentivized to predict correctly on the rare class -- specializing on the overrepresented classes will result in low average loss. \n",
    "\n",
    "We can ***reweight*** the data so that contributions from error associated with the rare class is increased while the error associated with the overrepresented classes is decreased. Intuitively, the model is penalized more for being 'wrong' on the rare class.\n",
    "\n",
    "<img src=\"fig/fig8.png\" alt=\"\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reweighting in `sklearn`\n",
    "\n",
    "Use the the `class_weight` parameter in `sklearn`'s' `LogisticRegression` model to put more emphasis on the smaller class. For example, \n",
    "```python \n",
    "# define a dictionary specifying the emphasis given to each class\n",
    "weights = {0: 10, 1: 1}\n",
    "\n",
    "# make an instance of the linear regression model, alpha is the strength of the penalty, class_weight is the relative weight of the two classes\n",
    "logistic = LogisticRegression(C=1., class_weight=weights)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression with Polynomial Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize a ellipitical decision boundary?\n",
    "\n",
    "<img src=\"fig/fig9.png\" alt=\"\" style=\"height: 300px;\"/>\n",
    "\n",
    "We can say that the decision boundary is given by a ***quadratic function*** of the input:\n",
    "$$\n",
    "w_1x^2_1 + w_2x^2_2 + w_3 = 0\n",
    "$$\n",
    "We say that we can fit such a decision boundary using logistic regression with degree 2 polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression with Quadratic Decision Boundary\n",
    "\n",
    "Recall that polynomial regression is simply a linear regression fit on polynomial features of the inputs `x`:\n",
    "1. transform $x$ into [1, $x$, $x^2$, $x^3$, $x^4$, ... etc]\n",
    "2. fit linear regression on the polynomial features [1, $x$, $x^2$, $x^3$, $x^4$, ... etc]\n",
    "\n",
    "Similary, if we want to fit a logistic regression with quadratic features, we:\n",
    "1. transform $x$ into [1, $x$, $x^2$]\n",
    "2. fit logistic regression on the quadratic features [1, $x$, $x^2$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression with Polynomial Boundary Implementation in `sklearn`\n",
    "\n",
    "``` python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#transform your training and testing data into polynomial features\n",
    "polynomial_features = PolynomialFeatures(2)\n",
    "polynomial_features.fit(x)\n",
    "x_poly = polynomial_features.transform(x)\n",
    "\n",
    "#fit a logistic regression on top of your polynomial features\n",
    "logistic_poly = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logistic_poly.fit(x_poly, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize an arbitrary complex decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig10.png\" style='height:300px;'>\n",
    "\n",
    "It's not easy to think of a function $g(x)$ can capture this decision boundary.\n",
    "\n",
    "**GOAL:** Find models that can capture *arbitrarily complex* decision boundaries."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
