{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Many Many Models for Classification\n",
    "## Introduction to Data Science\n",
    "### Kigali, Rwanda\n",
    "### July 9th, 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/logos.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. Polynomial Logistic Regression\n",
    "2. Decision Trees\n",
    "3. Bias and Variance\n",
    "4. Ensembling:\n",
    " - Random Forests\n",
    " - AdaBoost\n",
    "5. Neural Networks\n",
    "6. Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression with Polynomial Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Decision Boundaries\n",
    "\n",
    "When the decision boundary is linear, it is defined by the equation\n",
    "$$\n",
    "\\mathbf{w}^\\top \\mathbf{x} = w_0x_0 + w_1x_1 + \\ldots + w_D x_D = 0\n",
    "$$\n",
    "where $x_0 = 1$. Often we write $b = w_0x_0$ and we call $b$ the ***bias*** term.\n",
    "\n",
    "The vector $\\mathbf{w}$ allow us to gauge the 'distance' of a point from the decision boundary\n",
    "<img src=\"fig/fig0.png\" alt=\"\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize a ellipitical decision boundary?\n",
    "\n",
    "<img src=\"fig/fig1.png\" alt=\"\" style=\"height: 300px;\"/>\n",
    "\n",
    "We can say that the decision boundary is given by a ***quadratic function*** of the input:\n",
    "$$\n",
    "w_1x^2_1 + w_2x^2_2 + w_3 = 0\n",
    "$$\n",
    "We say that we can fit such a decision boundary using logistic regression with degree 2 polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression with Quadratic Decision Boundary\n",
    "\n",
    "Recall that polynomial regression is simply a linear regression fit on polynomial features of the inputs `x`:\n",
    "1. transform $x$ into [1, $x$, $x^2$, $x^3$, $x^4$, ... etc]\n",
    "2. fit linear regression on the polynomial features [1, $x$, $x^2$, $x^3$, $x^4$, ... etc]\n",
    "\n",
    "Similary, if we want to fit a logistic regression with quadratic features, we:\n",
    "1. transform $x$ into [1, $x$, $x^2$]\n",
    "2. fit logistic regression on the quadratic features [1, $x$, $x^2$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression with Polynomial Boundary Implementation in `sklearn`\n",
    "\n",
    "``` python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "#transform your training and testing data into polynomial features\n",
    "polynomial_features = PolynomialFeatures(2)\n",
    "polynomial_features.fit(x)\n",
    "x_poly = polynomial_features.transform(x)\n",
    "\n",
    "#fit a logistic regression on top of your polynomial features\n",
    "logistic_poly = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "logistic_poly.fit(x_poly, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize an arbitrary complex decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig2.png\" style='height:300px;'>\n",
    "\n",
    "It's not easy to think of a function $g(x)$ can capture this decision boundary.\n",
    "\n",
    "**GOAL:** Find models that can capture *arbitrarily complex* decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree Models\n",
    "\n",
    "People in every walk of life have long been using ***decision trees*** (flow charts) for differentiating between classes of objects and phenomena:\n",
    "<img src=\"./fig/fig28.png\" alt=\"\" style=\"height: 300px;\"/>\n",
    "Every flow chart tree corresponds to a partition of the input space by axis aligned lines or (hyper) planes.\n",
    "\n",
    "Conversely, every such partition can be written as a flow chart tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shallow vs Deep Trees: The Bias Variance Trade-Off\n",
    "**Model 1**: `train_accuracy = 0.653`, `test_accuracy = 0.60`\n",
    "\n",
    "**Model 2**: `train_accuracy = 0.993`, `test_accuracy = 0.7219`\n",
    "<img src=\"./fig/fig26.png\" style='height:350px;'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization: Increase Bias, Decrease Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization for Parametric Models\n",
    "\n",
    "Penalize the parameters of the model for being too large.\n",
    "\n",
    "1. Regression\n",
    "  - Linear regression with $\\ell_1$ regularization is called the `Lasso` regression in `sklearn`.\n",
    "  - Linear regression with $\\ell_2$ regularization is called the `Ridge` regression in `sklearn`.\n",
    "2. Classification\n",
    "  - Set the `C` parameter in `linear_model.LogisticRegression(C=1.)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization for Trees\n",
    "\n",
    "Limit the depth of the tree.\n",
    "\n",
    "<img src=\"./fig/fig27.png\" style='height:350px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensembling: How to Have Your Cake and Eat It Too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging: Random Forest\n",
    "\n",
    "As we've seen:\n",
    "\n",
    "1. a shallow decision tree can't capture complex decision boundaries (**high bias**)\n",
    "2. a deep decision tree is too sensitive to the noise in the data leading to overfitting (**high variance**)\n",
    "\n",
    "A compromise for reducing variance is to fit a large number of sensitive models (deep trees) on the training data and then average the results (reducing the variance).\n",
    "\n",
    "<img src=\"./fig/fig3.jpg\" alt=\"\" style=\"height: 250px;\"/>\n",
    "\n",
    "A **random forest** is the 'averaged model' of collection of deep decision trees each learned on a subset of the training data (each branch in each tree is also trained on a randomized set of input dimensions to reduce correlation between trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "Instead of 'averaging' over a large number of complex models (to reduce variance or overfitting), we can aggregate a large number of simple (low variance) models into a complex model (to overcome high bias).\n",
    "\n",
    "Each model $T_h$ might be a poor fit for the data, but a linear combination of the ensemble\n",
    "$$\n",
    "T = \\sum_h \\lambda_h T_h\n",
    "$$\n",
    "can be expressive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "***Gradient boosting*** is a method for iteratively building a complex regression model $T$ by adding simple models. Each new simple model added to the ensemble compensates for the weaknesses of the current ensemble.\n",
    "\n",
    "1. We start by fitting a simple model $T^{(0)}$ on the training data, $\\{(x_1, y_1),\\ldots, (x_N, y_N)\\}$.\n",
    "2. Compute the ***residuals*** or errors $\\{r_1, \\ldots, r_N\\}$ for $T$\n",
    "3. Fit a simple model $T^{(i)}$ to models the errors of $T$\n",
    "4. Set $T\\leftarrow T + \\lambda T^{i}$\n",
    "\n",
    "\n",
    "Intuitively, with each addition of $T^{(i)}$, the error is reduced \n",
    "\n",
    "Note that gradient boosting has a hyper-parameter, $\\lambda$. This is called the ***learning rate***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overwhelming Choices\n",
    "\n",
    "Now that you know so many models for classification, which model should you choose? Suppose that the classes are balanced in the below.\n",
    "\n",
    "<img src=\"./fig/fig4.png\" alt=\"\" style=\"height: 250px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remember Class Imbalance\n",
    "\n",
    "Your model must address class imbalance!\n",
    "\n",
    "Every `sklearn` classification model has a `class_weights` parameter you can set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Remember Computation Considerations\n",
    "\n",
    "1. Your model might need to run on small inefficient computers.\n",
    "\n",
    "2. Your prediction might need to be made in a brief period of time.\n",
    "\n",
    "3. You may need to make constant changes to your model.\n",
    "\n",
    "4. You may need to work with large quantities of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize an arbitrary complex decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig2.png\" style='height:300px;'>\n",
    "\n",
    "It's not easy to think of a function $g(x)$ can capture this decision boundary.\n",
    "\n",
    "**GOAL:** Find models that can capture *arbitrarily complex* decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximating Arbitrarily Complex Decision Boundaries\n",
    "\n",
    "Given an exact parametrization, we could learn the functional form, $g$, of the decision boundary directly. \n",
    "\n",
    "However, assuming an exact form for $g$ is restrictive. \n",
    "\n",
    "Rather, we can build increasingly good approximations, $\\widehat{g}$, of $g$ by composing simple functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neuron?\n",
    "\n",
    "**Goal:** build a good approximation $\\widehat{g}$ of a complex function $g$ by composing simple functions.\n",
    "\n",
    "For example, let the following picture represents $f\\left(\\sum_{i}w_ix_i\\right)$, where $f$ is a non-linear transform:\n",
    "\n",
    "<img src=\"./fig/fig5.png\" style='height:200px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neural Network?\n",
    "\n",
    "Translate the following graphical representation of a neural network into a functional form, $g(x_1, x_2) = ?$ \n",
    "<img src=\"./fig/fig6.png\" style='height:100px;'>\n",
    "Use the following labels:\n",
    "1. the input nodes $x_1$ and $x_2$\n",
    "2. the hidden nodes $h_1$ and $h_2$\n",
    "3. the output node $y$\n",
    "4. the weight from $x_i$ to $h_j$ as $w^i_j$\n",
    "5. the weight from $h_j$ to $y$ as $w^j$\n",
    "6. the activation function $f(x) = e^{-0.5x^2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks as Function Approximators\n",
    "\n",
    "Then we can define the approximation $\\widehat{g}$ with a graphical schema representing a complex series of compositions and sums of the form, $f\\left(\\sum_{i}w_ix_i\\right)$\n",
    "\n",
    "<img src=\"./fig/fig7.png\" style='height:300px;'>\n",
    "\n",
    "This is a ***neural network***. We denote the weights of the neural network collectively by $\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Flexible Framework for Function Approximation\n",
    "\n",
    "<img src=\"./fig/fig8.png\" style='height:500px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks for Prediction\n",
    "\n",
    "1. **Regression:** use features `x_train` to predict real-valued labels `y_train`.\n",
    "\n",
    "   **Neural Network Model:** `y_predict` $= g_\\mathbf{W}($ `x_train` $)$, where $g_\\mathbf{W}$ is a neural network with parameters $\\mathbf{W}$.\n",
    "\n",
    "   **Training Objective:** find $\\mathbf{W}$ to minimize the Mean Square Error, $\\min_{\\mathbf{W}}\\, \\mathrm{MSE}(\\mathbf{W})$.\n",
    "\n",
    "   **Optimizing the Training Objective:** How do we take the gradient of a neural network? Can we solve for the zeros of this gradient? \n",
    "<br>\n",
    "<br>\n",
    "2. **Classification:** use features `x_train` to predict categorical labels `y_train`.\n",
    "\n",
    "   **Neural Network Model:** $p(y=1 | $ `x_train` $ )= \\mathrm{sigmoid}(g_\\mathbf{W}($ `x_train` $))$, where $g_\\mathbf{W}$ is a neural network with parameters $\\mathbf{W}$.\n",
    "\n",
    "   **Training Objective:** find $\\mathbf{W}$ to minimize the **probability** of predicting wrong.\n",
    "\n",
    "   **Optimizing the Training Objective:** How do we take the gradient of a neural network? Can we solve for the zeros of this gradient? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent for Training Neural Networks:\n",
    "The intuition behind various flavours of gradient descent  is as follows:\n",
    "\n",
    "<img src=\"./fig/fig9.pdf\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent: the Algorithm\n",
    "1. start at random place: $W_0\\leftarrow \\textbf{random}$\n",
    "\n",
    "2. until (stopping condition satisfied):\n",
    "\n",
    "  a. compute gradient: \n",
    "     gradient = $\\nabla$ loss\\_function($W_{t}$)\n",
    "\n",
    "  b. take a step in the negative gradient direction: \n",
    "     $W_{t+1} \\leftarrow W_{t}$ - eta * gradient\n",
    "\n",
    "Here *eta* is called the ***learning rate***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks in `python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `keras`: a Python Library for Neural Networks\n",
    "\n",
    "`keras` is a `python` library that provides intuitive api's for build neural networks quickly. \n",
    "\n",
    "``` python\n",
    "#keras model for feedforward neural networks\n",
    "from keras.models import Sequential\n",
    "\n",
    "#keras model for layers in feedforward networks\n",
    "from keras.layers import Dense\n",
    "\n",
    "#keras model for optimizing training objectives\n",
    "from keras import optimizers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Neural Network for Classification in `keras`\n",
    "\n",
    "``` python\n",
    "#instantiate a feedforward model\n",
    "model = Sequential()\n",
    "\n",
    "#add layers sequentially\n",
    "\n",
    "#input layer: 2 input dimensions\n",
    "model.add(Dense(3, input_dim=2, activation='relu')) \n",
    "#hidden layer: 2 nodes\n",
    "model.add(Dense(2, activation='relu')) \n",
    "\n",
    "#output layer: 1 output dimension\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "#configure the model: specify training objective and training algorithm\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='binary_crossentropy')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Diagnosing Problems with Your Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monitoring Neural Network Training\n",
    "\n",
    "Visualize the mean square error over the training, this is called the training ***trajectory***.\n",
    "\n",
    "``` python\n",
    "#fit the model and return the mean squared error during training\n",
    "history = model.fit(x_train, y_train, batch_size=20, shuffle=True, epochs=500, verbose=0)\n",
    "\n",
    "# Plot the loss function and the evaluation metric over the course of training\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(np.array(history.history['loss']), color='blue', label='training loss function')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./fig/fig14.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./fig/fig15.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./fig/fig16.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Your NN: Optimization Choices Matter\n",
    "<img src=\"./fig/fig17.jpg\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dealing with Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do we Represent Gray-scale Images as Numerical Data?\n",
    "\n",
    "<img src=\"./fig/fig19.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent an Image as a Vector?\n",
    "<img src=\"./fig/fig20.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent an Image as a Vector?\n",
    "<img src=\"./fig/fig21.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent an Image as a Vector?\n",
    "<img src=\"./fig/fig22.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent an Image as a Vector?\n",
    "<img src=\"./fig/fig23.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent an Image as a Vector?\n",
    "\n",
    "This image, when flattened, is represented as a numpy array of shape `(441, )`.\n",
    "<img src=\"./fig/fig24.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How Do We Represent a Color Image Numerically?\n",
    "<img src=\"./fig/fig25.png\" style='height:250px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise:"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
